{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import config\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from csv import writer\n",
    "\n",
    "\n",
    "def gen_point_data(name, lat, lon, t_start, t_end):\n",
    "    '''\n",
    "    Generate particulate PM2.5 data for a lat/lon point over a set time period and write to csv\n",
    "    @params: \n",
    "        name: name of location\n",
    "        lat, lon: latitude and longitude in decimal degrees\n",
    "        t_start, t_end: starting and ending epoch in Unix time\n",
    "    '''\n",
    "\n",
    "    # Connect to endpoint and load data\n",
    "    endpoint = 'http://api.openweathermap.org/data/2.5/air_pollution/history?lat={LAT}&lon={LON}&start={START}&end={END}&appid={KEY}'.format(\n",
    "        LAT=lat, \n",
    "        LON=lon,\n",
    "        START=t_start,\n",
    "        END=t_end,\n",
    "        KEY=config.OPEN_WEATHER_KEY\n",
    "    )\n",
    "    page = requests.get(url=endpoint)\n",
    "    content = json.loads(page.content)\n",
    "#     df = pd.json_normalize(content)\n",
    "#     output = json.dumps(content, indent=2)\n",
    "    return content\n",
    "    # List all records\n",
    "#     ls = df['list'][0]\n",
    "#     df_size = len(ls)\n",
    "    \n",
    "#     # Take daily averages of PM2.5 particulate and add to list\n",
    "#     pm_list = []\n",
    "#     pm_count = 0\n",
    "#     for i in range(df_size):\n",
    "#         pm_count+=ls[i]['components']['pm2_5']\n",
    "#         # Average for each day\n",
    "#         if (i%24 == 0):   \n",
    "#             pm_daily = round(pm_count/24, 5)\n",
    "#             pm_list.append(pm_daily)\n",
    "#             pm_count = 0\n",
    "#     return pm_list\n",
    "\n",
    "### Retrieve data for list of cities ###\n",
    "content = {}\n",
    "city_df = pd.read_csv('C:\\\\Users\\\\Kurly\\\\Downloads\\\\Universal-Embeddings-Nick\\\\Universal-Embeddings-Nick\\\\data\\\\city_lat_lon.csv')\n",
    "city_count = 65 # Actual: len(city_df)\n",
    "\n",
    "# Start and ending times. Testing for Dec 2020\n",
    "T_START = 1606456800\n",
    "T_END = 1623128400\n",
    "for i in range(city_count): \n",
    "    if i < (65-5) :\n",
    "       pass \n",
    "    else:\n",
    "        city_name = city_df.iloc[i][0]\n",
    "        city_lat = city_df.iloc[i][1]\n",
    "        city_lon = city_df.iloc[i][2]\n",
    "        city_info=[city_name, city_lat, city_lon]\n",
    "        # Retrieve particulate list; write row to csv\n",
    "        entry = gen_point_data(name=city_name, lat=city_lat, lon=city_lon, t_start=T_START, t_end=T_END)\n",
    "        content[city_name]=entry\n",
    "        print(i)\n",
    "with open('raw_data.txt', 'w') as outfile:\n",
    "    json.dump(content, outfile)\n",
    "\n",
    "# Derive number of entries from start and end\n",
    "# Change in epoch to number of hours gets us total entries\n",
    "# num_entries = int((T_END - T_START) / 86400)\n",
    "# time_step = T_START\n",
    "\n",
    "# # Get list of column names based on the number of entries (each hour of data will be one column)\n",
    "# col_names = ['city', 'lat', 'lon']\n",
    "# for i in range(num_entries):\n",
    "#     # Convert daily interval to human-readable\n",
    "#     timedate = datetime.datetime.fromtimestamp(time_step)\n",
    "#     time_string = timedate.strftime('pm25_%Y_%m_%d')\n",
    "#     # Increment time_step\n",
    "#     time_step+=86400\n",
    "#     # Append col to list\\\n",
    "#     col_names.append(time_string)\n",
    "\n",
    "# Write entry to file\n",
    "# with open('C:\\\\Users\\\\Kurly\\\\Downloads\\\\Universal-Embeddings-Nick\\\\Universal-Embeddings-Nick\\\\data\\\\geocoded-cities-master.csv', 'w', newline='') as f_open:\n",
    "#     writer_obj = writer(f_open)\n",
    "#     # Write header\n",
    "#     writer_obj.writerow(col_names)\n",
    "\n",
    "#     # Loop through all 28,000+ cities and retrieve data\n",
    "#     for i in range(city_count): \n",
    "#         city_name = city_df.iloc[i][0]\n",
    "#         city_lat = city_df.iloc[i][1]\n",
    "#         city_lon = city_df.iloc[i][2]\n",
    "#         city_info=[city_name, city_lat, city_lon]\n",
    "#         # Retrieve particulate list; write row to csv\n",
    "#         entry = gen_point_data(name=city_name, lat=city_lat, lon=city_lon, t_start=T_START, t_end=T_END)\n",
    "#         city_info+=entry\n",
    "#         writer_obj.writerow(city_info)\n",
    "#         print(i, entry[1])\n",
    "            \n",
    "#     f_open.close()\n",
    "\n",
    "# one=pd.read_csv('C:\\\\Users\\\\Kurly\\\\Downloads\\\\Universal-Embeddings-Nick\\\\Universal-Embeddings-Nick\\\\data\\\\geocoded-cities-master.csv', encoding='latin-1')\n",
    "# one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Dallas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-63a78dc10332>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#df = pd.json_normalize(lines)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Dallas'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Dallas'"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('raw_data.txt') as f:\n",
    "    lines = json.load(f)\n",
    "f.close()\n",
    "print(len(lines))\n",
    "#df = pd.json_normalize(lines)\n",
    "output = json.dumps(lines['Dallas'], indent=2)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
